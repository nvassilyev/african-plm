{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, XmodConfig, XmodModel, XmodPreTrainedModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmod = AutoModelForMaskedLM.from_pretrained(\"facebook/xmod-base\")\n",
    "xmod_75 = AutoModelForMaskedLM.from_pretrained(\"facebook/xmod-base-75-269k\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"facebook/xmod-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGES = ['en_XX', 'ar_AR', 'fr_XX', 'sw_KE', 'af_ZA', 'am_ET', 'ha_NG', 'pt_XX', 'so_SO']\n",
    "LANGUAGES_75 = ['om_KE', 'xh_ZA', 'mg_MG']\n",
    "\n",
    "ref = ['id_ID', 'vi_VN', 'ru_RU', 'fa_IR', 'sv_SE', 'ja_XX', 'de_DE', 'ro_RO', 'ko_KR',\\\n",
    "     'hu_HU', 'es_XX', 'fi_FI', 'uk_UA', 'da_DK', 'no_XX', 'th_TH', 'pl_PL', 'bg_BG', 'nl_XX',\\\n",
    "         'zh_CN', 'he_IL', 'el_GR', 'it_IT', 'sk_SK', 'hr_HR', 'tr_TR', 'cs_CZ', 'lt_LT', 'hi_IN',\\\n",
    "             'zh_TW', 'ca_ES', 'ms_MY', 'sl_SI', 'lv_LV', 'ta_IN', 'bn_IN', 'et_EE', 'az_AZ', 'sq_AL',\\\n",
    "                 'sr_RS', 'kk_KZ', 'ka_GE', 'tl_XX', 'ur_PK', 'is_IS', 'hy_AM', 'ml_IN', 'mk_MK', 'be_BY',\\\n",
    "                     'la_VA', 'te_IN', 'eu_ES', 'gl_ES', 'mn_MN', 'kn_IN', 'ne_NP', 'si_LK', 'mr_IN', 'gu_IN',\\\n",
    "                         'cy_GB', 'eo_EO', 'km_KH', 'ky_KG', 'uz_UZ', 'ps_AF', 'pa_IN', 'ga_IE', 'lo_LA', 'ku_TR',\\\n",
    "                             'my_MM', 'or_IN', 'sa_IN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune languages we do not need\n",
    "\n",
    "xmod_langs = list(xmod.roberta.encoder.layer[0].output.adapter_modules.keys())\n",
    "prune = [lang for lang in xmod_langs if lang not in LANGUAGES]\n",
    "\n",
    "for lang in prune:\n",
    "    for layer in model.roberta.encoder.layer:\n",
    "        if lang in layer.output.adapter_modules.keys():\n",
    "            layer.output.adapter_modules.pop(lang)\n",
    "\n",
    "# add missing adapters from xmod_75 model\n",
    "\n",
    "for lang in LANGUAGES_75:\n",
    "    for i in range(model.config.num_hidden_layers):\n",
    "        adapters_base = model.roberta.encoder.layer[i].output.adapter_modules\n",
    "        adapters_75 = xmod_75.roberta.encoder.layer[i].output.adapter_modules\n",
    "        adapters_base[lang] = adapters_75[lang]\n",
    "\n",
    "model.config.languages = LANGUAGES + LANGUAGES_75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeding layer\n",
      "Parameter: position_ids\t Shape: torch.Size([1, 514])\n",
      "Parameter: word_embeddings.weight\t Shape: torch.Size([250002, 768])\n",
      "Parameter: position_embeddings.weight\t Shape: torch.Size([514, 768])\n",
      "Parameter: token_type_embeddings.weight\t Shape: torch.Size([1, 768])\n",
      "Parameter: LayerNorm.weight\t Shape: torch.Size([768])\n",
      "Parameter: LayerNorm.bias\t Shape: torch.Size([768])\n",
      "Xmod layer\n",
      "Parameter: attention.self.query.weight\t Shape: torch.Size([768, 768])\n",
      "Parameter: attention.self.query.bias\t Shape: torch.Size([768])\n",
      "Parameter: attention.self.key.weight\t Shape: torch.Size([768, 768])\n",
      "Parameter: attention.self.key.bias\t Shape: torch.Size([768])\n",
      "Parameter: attention.self.value.weight\t Shape: torch.Size([768, 768])\n",
      "Parameter: attention.self.value.bias\t Shape: torch.Size([768])\n",
      "Parameter: attention.output.dense.weight\t Shape: torch.Size([768, 768])\n",
      "Parameter: attention.output.dense.bias\t Shape: torch.Size([768])\n",
      "Parameter: attention.output.LayerNorm.weight\t Shape: torch.Size([768])\n",
      "Parameter: attention.output.LayerNorm.bias\t Shape: torch.Size([768])\n",
      "Parameter: intermediate.dense.weight\t Shape: torch.Size([3072, 768])\n",
      "Parameter: intermediate.dense.bias\t Shape: torch.Size([3072])\n",
      "Parameter: output.dense.weight\t Shape: torch.Size([768, 3072])\n",
      "Parameter: output.dense.bias\t Shape: torch.Size([768])\n",
      "Parameter: output.LayerNorm.weight\t Shape: torch.Size([768])\n",
      "Parameter: output.LayerNorm.bias\t Shape: torch.Size([768])\n",
      "adapter\n",
      "Parameter: dense1.weight\t Shape: torch.Size([384, 768])\n",
      "Parameter: dense1.bias\t Shape: torch.Size([384])\n",
      "Parameter: dense2.weight\t Shape: torch.Size([768, 384])\n",
      "Parameter: dense2.bias\t Shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "state = model.roberta.encoder.layer[0].state_dict()\n",
    "emb = model.roberta.embeddings.state_dict()\n",
    "adapter = model.roberta.encoder.layer[0].output.adapter_modules['en_XX'].state_dict()\n",
    "\n",
    "print(\"Embedding layer\")\n",
    "for name, param in emb.items():\n",
    "    print(f\"Parameter: {name}\\t Shape: {param.shape}\")\n",
    "\n",
    "print(\"Xmod layer\")\n",
    "for name, param in state.items():\n",
    "    if \"adapter\" not in name:\n",
    "        print(f\"Parameter: {name}\\t Shape: {param.shape}\")\n",
    "\n",
    "print(\"adapter\")\n",
    "for name, param in adapter.items():\n",
    "    print(f\"Parameter: {name}\\t Shape: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0187, -0.0354,  0.0257,  ..., -0.0096,  0.0059, -0.0199],\n",
       "        [-0.0145,  0.0308,  0.0187,  ...,  0.0264, -0.0297, -0.0341],\n",
       "        [ 0.0263, -0.0214,  0.0282,  ..., -0.0146,  0.0359, -0.0242],\n",
       "        ...,\n",
       "        [ 0.0354, -0.0030,  0.0047,  ..., -0.0075,  0.0212,  0.0130],\n",
       "        [-0.0084, -0.0204,  0.0119,  ..., -0.0321, -0.0345,  0.0262],\n",
       "        [-0.0245, -0.0230, -0.0022,  ...,  0.0335,  0.0224,  0.0262]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize new languages\n",
    "from adapter import XmodAdapter\n",
    "\n",
    "new_lang = 'yo_XX'\n",
    "\n",
    "for i in range(model.config.num_hidden_layers):\n",
    "    new_adapter = XmodAdapter(model.config)\n",
    "    model.roberta.encoder.layer[i].output.adapter_modules[new_lang] = new_adapter\n",
    "\n",
    "if new_lang not in model.config.languages:\n",
    "    model.config.languages.append(new_lang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "XLMRobertaTokenizer.__init__() missing 1 required positional argument: 'vocab_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# New embedding space // need tokenizers to work for this \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m XLMRobertaTokenizer\n\u001b[0;32m----> 5\u001b[0m XLMRobertaTokenizer()\u001b[39m.\u001b[39mget_vocab()[:\u001b[39m10\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: XLMRobertaTokenizer.__init__() missing 1 required positional argument: 'vocab_file'"
     ]
    }
   ],
   "source": [
    "# New embedding space // need tokenizers to work for this \n",
    "\n",
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "XLMRobertaTokenizer().get_vocab()[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
